{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7974bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "import pyfftw\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.signal import peak_widths, medfilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac77320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_window(PSD):\n",
    "    \"\"\"Calculates window to calculate SNR for signal.\n",
    "\n",
    "    Args:\n",
    "        PSD: PSD: 1D array of injected PSD data.\n",
    "\n",
    "    Returns:\n",
    "        window: A tuple[peak, left, right] containing:\n",
    "            peak: index where peak of data is.\n",
    "            left: index where the left bin of the signal is.\n",
    "            right: index where the right bin of the signal is.\n",
    "    \"\"\"\n",
    "    # Applies median filter to data to remove any outliers\n",
    "    PSD = medfilt(PSD)  \n",
    "\n",
    "    # We calculate the window for the first pass of the function (injected PSD)\n",
    "    peak = np.argmax(PSD)\n",
    "    _, _, left, right = peak_widths(PSD, [peak], rel_height=0.7, wlen=20001)\n",
    "    window = (peak, int(left[0]), int(right[0]))\n",
    "\n",
    "    return window\n",
    "\n",
    "\n",
    "def get_snr(PSD, window):\n",
    "    \"\"\"Calculates SNR of PSD.\n",
    "\n",
    "    Args:\n",
    "        PSD: 1D array of PSD data.\n",
    "        window: A tuple[peak, left, right] containing:\n",
    "            peak: index where peak of data is.\n",
    "            left: index where the left bin of the signal is.\n",
    "            right: index where the right bin of the signal is.\n",
    "\n",
    "    Returns:\n",
    "        SNR: Signal-to-noise ratio of signal.\n",
    "    \"\"\"\n",
    "    \n",
    "    peak       = window[0]\n",
    "    left_edge  = window[1]\n",
    "    right_edge = window[2]\n",
    "    noise_bin  = 7500 #5000\n",
    "    \n",
    "    signal = np.mean(PSD[left_edge : right_edge + 1])\n",
    "\n",
    "    noise  = np.concatenate([PSD[left_edge - noise_bin : left_edge - 200],\n",
    "                             PSD[right_edge + 201      : right_edge + noise_bin + 1]])\n",
    "    \n",
    "    mean_noise = np.mean(noise)\n",
    "    noise_std  = np.std(noise)\n",
    "\n",
    "    snr = (signal - mean_noise) / noise_std\n",
    " \n",
    "    return snr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ee9d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plots SNRI as a function of SNR for different signal sizes\n",
    "\n",
    "# Setting directory to pull files from\n",
    "directory = Path('Denoised_fft_science/')\n",
    "file_list = sorted(directory.glob(f'*dcae.h5'))\n",
    "\n",
    "snr   = []\n",
    "snri  = []\n",
    "\n",
    "# Currently 59 files with signal size ranging from 0 to 1.5e3\n",
    "for i, filename in enumerate(file_list[:]):\n",
    "    f = h5py.File(filename, 'r')\n",
    "\n",
    "    # Data = log10(x + 1), so must do 10 ** (Data) - 1\n",
    "    noise    = np.exp(f['input'][:]) - 1 \n",
    "    denoised = np.exp(f['denoised'][:]) - 1\n",
    "    injected = np.exp(f['injected'][:]) - 1\n",
    "    \n",
    "    # Basing our signal window off the injected signal\n",
    "    window = get_window(injected) \n",
    "\n",
    "    # Calculating SNR for noise PSD and denoised PSD\n",
    "    noise_snr    = get_snr(noise, window)\n",
    "    denoised_snr = get_snr(denoised, window)\n",
    "\n",
    "    snr.append(noise_snr)\n",
    "    snri.append(denoised_snr / noise_snr)\n",
    "\n",
    "\n",
    "# Setting up linear fit to data\n",
    "\n",
    "# First point has no injected signal so don't count it\n",
    "# One thing to keep in mind is that the first few signals are really small and don't really give an accurate SNRI\n",
    "# change start >= 10 to get more accurate plot \n",
    "start = 10      \n",
    "snr   = np.array(snr[start:])\n",
    "snri  = np.array(snri[start:])\n",
    "m, b = np.polyfit(snr, snri, 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(snr, snri, label='Data')\n",
    "plt.plot(snr, snr * m + b, label = f'LOB | Slope: {m:.2f}', c='red')\n",
    "plt.ylabel('SNRI')\n",
    "plt.xlabel('SNR')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e044d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plots SNRI as a function of Scale factor (signal width)\n",
    "\n",
    "# Setting directory to pull files from\n",
    "directory = Path('Denoised_fft_science_signalWidth/')\n",
    "file_list = sorted(directory.glob(f'*dcae.h5'))\n",
    "\n",
    "snri  = []\n",
    "scale = []\n",
    "\n",
    "# Currently 49 files with scale_factor ranging from 0.5 to ~2.6\n",
    "for i, filename in enumerate(file_list[:]):\n",
    "    f = h5py.File(filename, 'r')\n",
    "\n",
    "    # Data = log10(x + 1), so must do 10 ** (Data) - 1\n",
    "    noise    = np.exp(f['input'][:]) - 1 \n",
    "    denoised = np.exp(f['denoised'][:]) - 1\n",
    "    injected = np.exp(f['injected'][:]) - 1\n",
    "    \n",
    "    scale_factor = f['injected'].attrs['scale_factor']\n",
    "    scale.append(scale_factor)\n",
    "    \n",
    "    # Basing our signal window off the injected signal\n",
    "    window = get_window(injected) \n",
    "\n",
    "    # Calculating SNR for noise PSD and denoised PSD\n",
    "    noise_snr    = get_snr(noise, window)\n",
    "    denoised_snr = get_snr(denoised, window)\n",
    "\n",
    "    snri.append(denoised_snr / noise_snr)\n",
    "\n",
    "scale = np.array(scale[:])\n",
    "snri  = np.array(snri[:])\n",
    "\n",
    "# Setting exponential function to fit the data\n",
    "def negative_exponential(x, A, k, C):\n",
    "    return A * np.exp(-k * x) + C\n",
    "\n",
    "# Initial guesses for parameters (A, k, C)\n",
    "p0 = [20, 2, 2] \n",
    "params, _ = curve_fit(negative_exponential, scale, snri, p0=p0)\n",
    "\n",
    "# Extracting the fitted parameters\n",
    "A_fit, k_fit, C_fit = params\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(scale, snri, label='Data')\n",
    "plt.plot(scale, negative_exponential(scale, A_fit, k_fit, C_fit), c='red', label=f'LOB')\n",
    "plt.ylabel('SNRI')\n",
    "plt.xlabel('Scale factor (signal width)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce59b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plots SNRI as function of integration time for different signal sizes\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "sampling_freq = int(400e3)\n",
    "batchsize     = 1\n",
    "input_size    = sampling_freq // 2\n",
    "\n",
    "def read_loader(ADMXfile):\n",
    "    \"\"\"Reads in Time-Series data and converts to scaled down PSD.\n",
    "\n",
    "    Args:\n",
    "        ADMXfile: Opened .h5 file with ['input'] and ['injected'] channels.\n",
    "\n",
    "    Returns:\n",
    "        train_loader: Concatenated array of train and target 1D arrays.\n",
    "        \n",
    "    \"\"\"\n",
    "    train = ADMXfile['input'][:]\n",
    "    train = train.reshape(-1, sampling_freq)\n",
    "    train = np.abs(pyfftw.interfaces.scipy_fft.rfft(train)[:, 1:]) ** 2\n",
    "    # Trained better with log1p than log10 \n",
    "    train = np.log1p(train)\n",
    "    train = train.reshape(-1, batchsize, input_size)\n",
    "    \n",
    "    target = ADMXfile['injected'][:]\n",
    "    target = target.reshape(-1, sampling_freq)\n",
    "    target = np.abs(pyfftw.interfaces.scipy_fft.rfft(target)[:, 1:]) ** 2\n",
    "    target = np.log1p(target)\n",
    "    target = target.reshape(-1, batchsize, input_size)\n",
    "\n",
    "    train_loader = np.concatenate([train, target], axis=1)\n",
    "\n",
    "    return train_loader\n",
    "\n",
    "# Loading in model \n",
    "model = torch.load(f'DCAE_PSD.pth', map_location=DEVICE, weights_only=False)\n",
    "model.eval()\n",
    "\n",
    "# Setting directory to pull files from\n",
    "directory = Path('integrationTime_Science/')\n",
    "file_list = sorted(directory.glob('*.h5'))\n",
    "\n",
    "# Range for integration time\n",
    "seconds = np.arange(0, 100)\n",
    "\n",
    "# Currently only 5 files with sig_size = [1.5e2, 1.5e3, 1.5e4, 1.5e5, 1.5e6]\n",
    "for ii, fname in tqdm(enumerate(file_list[:])): \n",
    "    ADMXfile     = h5py.File(fname,'r')\n",
    "    train_loader = read_loader(ADMXfile)\n",
    "    sig_size = ADMXfile['injected'].attrs['sig_size']\n",
    "\n",
    "    print(f'{fname.stem} with a signal size of {sig_size}')\n",
    "\n",
    "    noise    = []\n",
    "    denoised = []\n",
    "    injected = []\n",
    "    snri = []\n",
    "\n",
    "    # Inputting data into ML model and getting denoised output\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        inputarr, targetarr = (batch[:batchsize], batch[batchsize:])\n",
    "\n",
    "        input_seq  = torch.from_numpy(inputarr)\n",
    "        input_seq  = input_seq.float().to(DEVICE)\n",
    "\n",
    "        output_seq = model(input_seq).detach().cpu().numpy()\n",
    "        \n",
    "        # Appending the 3 arrays to their corresponding lists\n",
    "        denoised.append(output_seq.flatten())\n",
    "        injected.append(targetarr.flatten())\n",
    "        noise.append(inputarr.flatten())\n",
    "        \n",
    "    # Basing our window size after taking the stacked mean on injected signal\n",
    "    window = get_window(np.exp(np.mean(np.array(injected[:]), axis=0)) - 1)\n",
    "    \n",
    "    for i in seconds:\n",
    "        # Scaling back to regular values and taking mean from index 0 to i + 1 for a total of 100 seconds\n",
    "        # For each loop we add an extra second to average from\n",
    "        noise_psd    = np.exp(np.mean(np.array(noise[:i + 1]), axis=0)) - 1\n",
    "        denoised_psd = np.exp(np.mean(np.array(denoised[:i + 1]), axis=0)) - 1\n",
    "\n",
    "        noise_snr    = get_snr(noise_psd, window)\n",
    "        denoised_snr = get_snr(denoised_psd, window)\n",
    "\n",
    "        snri.append(denoised_snr / noise_snr)\n",
    "    \n",
    "    plt.plot(seconds, snri, label=f'signal size = {sig_size}')\n",
    "    plt.xlabel('Integration Time (s)')\n",
    "    plt.ylabel('SNRI')\n",
    "    plt.ylim((0, 35))\n",
    "    plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1c3691",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:admx_ml]",
   "language": "python",
   "name": "conda-env-admx_ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
